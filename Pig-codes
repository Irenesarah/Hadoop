#copy files to read
Hadoop fs –copyFromLocal emp.csv pig/emp.csv
#Load Data
A = load ‘/user/root/pig/emp.csv using PigStorage(‘,’) as (eid:int,ename:chararray,epos:chararray,esal:int,ecom:int,edpno:int);
Dump A;
A2 = load ‘/user/root/pig/emp.csv’;
Describe A2;
Aggregate(by row)
B=filter A by edpno ==20;
B2=filter A by edpno==20 and epos ==’MANAGER’,
C = limit B 3;
D = order C by esal desc;
#Store data
Store D into ‘/pig/pigout1’ using PigStorage(‘,’);
Transform (by column)
Selecting existing column
E = foreach A generate eid;
Create new column
F= foreach A generate *,ecom*2 as Bonus,esal*5 as Incentive;
Transform columns
G=foreach A generate SUBSTRING(ename,0,4);
H = foreach A generate $0,$1;
I =group A by edpno;
J=foreach I generate group as edpno,COUNT($1) as count;
K = foreach A generate MAX(A.esal) as maxsal,MIN(A.esal) as minsal,SUM(A.esal) as sumsal,COUNT($1) as count;
L = group A by (edpno,epos)
SPLIT A  into B if edpno ==10,C if edpno==20,D if epos==’MANAGER’;
JOINS
A = load ‘/user/root/pig/emp.csv using PigStorage(‘,’) as (eid:int,ename:chararray,epos:chararray,esal:int,ecom:int,edpno:int);
B = load ‘/dept.csv using PigStorage(‘,’) as (eid:int,ename:chararray,epos:chararray,esal:int,ecom:int,edpno:int);
C = JOIN A by edpno,B by edpno;
D = foreach C generate A::eid,B::epos;
E = JOIN A by edpno RIGHT OUTER ,B by edpno;

